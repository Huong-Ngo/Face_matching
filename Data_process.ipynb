{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from train_arcface import train_arcface\n",
    "from src.utils.model import get_triplet_model, freeze_layer\n",
    "from src.model.arcface import KSubArcFace, ArcFace_org\n",
    "import torch\n",
    "from train_triplet import train_triplet\n",
    "from train_arc_triplet import train_arc_triplet\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_PATH = 'weight/backbone_r100_glint360k.pt'\n",
    "TRAIN_PATH = \"Data/train_triplet_processed.txt\"\n",
    "TEST_PATH = \"Data/train_triplet_processed.txt\"\n",
    "EPOCHS = 10\n",
    "BATCH__SIZE = 64\n",
    "\n",
    "# FINETUNE_PATH = 'weight/model_finetune_arcface_org_best.pth'\n",
    "ARCFACE_PATH = 'weight/arcface_epoch=66500_val_F1=0.9624_last.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 123] The filename, directory name, or volume label syntax is incorrect: 'runs/arcface_s=30_m=1.0_2023-07-10 16:16:25.658359'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m arcface \u001b[39m=\u001b[39m ArcFace_org(in_features \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m, out_features \u001b[39m=\u001b[39m \u001b[39m18948\u001b[39m, s \u001b[39m=\u001b[39m \u001b[39m30\u001b[39m, m \u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m)\n\u001b[0;32m      5\u001b[0m arcface\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(ARCFACE_PATH, map_location \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mmodel_checkpoint\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 8\u001b[0m train_arcface(model, arcface, TRAIN_PATH, TEST_PATH, EPOCHS, BATCH__SIZE, checkpoint_path\u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mweight_triplet_arc\u001b[39;49m\u001b[39m'\u001b[39;49m )\n",
      "File \u001b[1;32md:\\Projects\\FaceMatching\\train_arcface.py:80\u001b[0m, in \u001b[0;36mtrain_arcface\u001b[1;34m(model, arcface, train_path, test_path, epochs, batch_size, device, checkpoint_path)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_arcface\u001b[39m(model, arcface, train_path, test_path, epochs, batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m, device \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, checkpoint_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mweight_arc_triplet\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m---> 80\u001b[0m     writer \u001b[39m=\u001b[39m SummaryWriter(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mruns/arcface_s=\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mstr\u001b[39;49m(arcface\u001b[39m.\u001b[39;49ms)\u001b[39m}\u001b[39;49;00m\u001b[39m_m=\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mstr\u001b[39;49m(arcface\u001b[39m.\u001b[39;49mm)\u001b[39m}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00mdate_time\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     81\u001b[0m     \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     82\u001b[0m         device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:247\u001b[0m, in \u001b[0;36mSummaryWriter.__init__\u001b[1;34m(self, log_dir, comment, purge_step, max_queue, flush_secs, filename_suffix)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[39m# Initialize the file writers, but they can be cleared out on close\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[39m# and recreated later as needed.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_writers \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_file_writer()\n\u001b[0;32m    249\u001b[0m \u001b[39m# Create default bins for histograms, see generate_testdata.py in tensorflow/tensorboard\u001b[39;00m\n\u001b[0;32m    250\u001b[0m v \u001b[39m=\u001b[39m \u001b[39m1e-12\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:277\u001b[0m, in \u001b[0;36mSummaryWriter._get_file_writer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns the default FileWriter instance. Recreates it if closed.\"\"\"\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_writers \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 277\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer \u001b[39m=\u001b[39m FileWriter(\n\u001b[0;32m    278\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_dir, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_queue, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflush_secs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilename_suffix\n\u001b[0;32m    279\u001b[0m     )\n\u001b[0;32m    280\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_writers \u001b[39m=\u001b[39m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer\u001b[39m.\u001b[39mget_logdir(): \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer}\n\u001b[0;32m    281\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpurge_step \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:76\u001b[0m, in \u001b[0;36mFileWriter.__init__\u001b[1;34m(self, log_dir, max_queue, flush_secs, filename_suffix)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39m# Sometimes PosixPath is passed in and we need to coerce it to\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39m# a string in all cases\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39m# TODO: See if we can remove this in the future if we are\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39m# actually the ones passing in a PosixPath\u001b[39;00m\n\u001b[0;32m     75\u001b[0m log_dir \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(log_dir)\n\u001b[1;32m---> 76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevent_writer \u001b[39m=\u001b[39m EventFileWriter(\n\u001b[0;32m     77\u001b[0m     log_dir, max_queue, flush_secs, filename_suffix\n\u001b[0;32m     78\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorboard\\summary\\writer\\event_file_writer.py:72\u001b[0m, in \u001b[0;36mEventFileWriter.__init__\u001b[1;34m(self, logdir, max_queue_size, flush_secs, filename_suffix)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a `EventFileWriter` and an event file to write to.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \n\u001b[0;32m     59\u001b[0m \u001b[39mOn construction the summary writer creates a new event file in `logdir`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39m    pending events and summaries to disk.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logdir \u001b[39m=\u001b[39m logdir\n\u001b[1;32m---> 72\u001b[0m tf\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mgfile\u001b[39m.\u001b[39;49mmakedirs(logdir)\n\u001b[0;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file_name \u001b[39m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m     75\u001b[0m         logdir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[39m+\u001b[39m filename_suffix\n\u001b[0;32m     85\u001b[0m )  \u001b[39m# noqa E128\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_general_file_writer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mGFile(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file_name, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\io\\gfile.py:907\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmakedirs\u001b[39m(path):\n\u001b[0;32m    900\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Creates a directory and all parent/intermediate directories.\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \n\u001b[0;32m    902\u001b[0m \u001b[39m    It succeeds if path already exists and is writable.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[39m      path: string, name of the directory to be created\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 907\u001b[0m     \u001b[39mreturn\u001b[39;00m get_filesystem(path)\u001b[39m.\u001b[39;49mmakedirs(path)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\io\\gfile.py:208\u001b[0m, in \u001b[0;36mLocalFileSystem.makedirs\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmakedirs\u001b[39m(\u001b[39mself\u001b[39m, path):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Creates a directory and all parent/intermediate directories.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     os\u001b[39m.\u001b[39;49mmakedirs(path, exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 123] The filename, directory name, or volume label syntax is incorrect: 'runs/arcface_s=30_m=1.0_2023-07-10 16:16:25.658359'"
     ]
    }
   ],
   "source": [
    "model = get_triplet_model(weight_path= WEIGHT_PATH)\n",
    "# model.backbone.load_state_dict(torch.load(FINETUNE_PATH, map_location = 'cpu')['model_state_dict'])\n",
    "\n",
    "arcface = ArcFace_org(in_features = 512, out_features = 18948, s = 30, m = 1.)\n",
    "arcface.load_state_dict(torch.load(ARCFACE_PATH, map_location = 'cpu')['model_checkpoint'])\n",
    "\n",
    "\n",
    "train_arcface(model, arcface, TRAIN_PATH, TEST_PATH, EPOCHS, BATCH__SIZE, checkpoint_path= 'weight_arcface_10_7' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_triplet_model(weight_path= WEIGHT_PATH)\n",
    "# freeze_layer(model, ['253', '254'])\n",
    "# train_triplet(model, TRAIN_PATH, TEST_PATH, EPOCHS, BATCH__SIZE, roc_fig_path='result/finetune_triplet_loss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import TripletFaceDataset, Triplet_loader \n",
    "import torch\n",
    "test_dataset= Triplet_loader(\"Data/valid_triplet_processed.txt\")\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=int(.75 * 64),\n",
    "                                        shuffle=False, num_workers= 0, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import TripletFaceDataset, Triplet_loader\n",
    "from src.utils.model import get_test_model\n",
    "\n",
    "val_model = get_test_model(weight_path= None,  init_weight = \"weight/backbone_r100_glint360k.pt\")\n",
    "val_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [02:08<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.3140852451324463\n",
      "precision: 0.9752702951330642\n",
      "recall: 0.967054429945055\n",
      "F1 score: 0.9711449863403436\n",
      "Accuracy: 0.9712665264423077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9711449863403436, 0.4558490773478707)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from test import validation\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sim_loss(x1,x2):\n",
    "    return torch.clamp_min(1 - F.cosine_similarity(x1,x2),0)\n",
    "\n",
    "validation(val_model,  test_loader = testloader, criterion =  nn.TripletMarginWithDistanceLoss(distance_function = sim_loss, margin=1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
